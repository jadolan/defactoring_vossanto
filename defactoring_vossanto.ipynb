{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defactoring Vossanto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Arbeit wird der Code zum Paper *\"The Michael Jordan of greatness\". Extracting Vossian antonomasia from two decades of The New York Times, 1987-2007* von Fischer und Jäschke aus dem Jahr 2019 untersucht. Der für diese Studie verwendete Code wurde in einem [Github Repositorium](https://github.com/weltliteratur/vossanto/tree/master/theof) veröffentlicht. Sofern nicht explizit anderweitig erwähnt, stammt sämtlicher in diesem Notebook verwendeter Code aus dem erwähnten Repositorium.\n",
    "Der Code ist überwiegend in der Programmiersprache Python geschrieben. Zur Veranschaulichung von Zwischenergebnissen werden außerdem Shellbefehle verwendet. So können Textdateien mit der Skriptsprache *awk* direkt im Dateisystem ausgewertet werden. Das Notebook folgt damit der Methodik von Fischer und Jäschke zur Veranschaulichung der Daten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung\n",
    "\n",
    "In diesem Abschnitt werden Vorbereitungen für die Analyse der Daten vorgenommen. Dazu gehört u. a. das Laden benötigter Bibliotheken. Dieser Schritt stellt eine Schnittstelle zwischen der projektspezifischen Software, welche gezielt für diese Studie geschrieben wurde, und allgemeinerer Software und Infrastruktur dar. Diese Brücke wird durch das Schlüsselwort *import* deutlich, mit dem die Funktionalität eines existierenden Programms in das hier geschriebene eingefügt wird und wiederverwendet werden kann.\n",
    "\n",
    "Anschließend werden in diesem Auswertungsschritt benötigte Parameter wie Dateipfade und verwendete Muster regulärer Ausdrücke (Regex) konfiguriert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFACTORING IMPORT\n",
    "\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import codecs\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Von den geladenen Bibliotheken kann die Mehrzahl der allgemeinen Softwareebene in [Hinsens Modell](http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/) zugeordnet werden. Dazu gehören *re*, *xml*, *os* und *codecs*. All diese sind Standardbibliotheken, die von Python bereit gestellt und nicht speziell im wissenschaftlichen Kontext angewandt werden. Die Bibliothek *nltk* gehört zur disziplinspezifischen Software. Sie ist eine weit verbreitete Bibliothek im Bereich *Natural Language Processing*. Für diesen Anwendungsfall wird lediglich die Methode *sent_tokenize* importiert, die Texte in Sätze aufteilt.  \n",
    "Allein die Betrachtung dieser Imports gibt bereits Hinweise auf die Art des erzeugten Programms. Ohne dass konkrete Anweisungen und Methoden für den Umgang mit Eingabewerten erkennbar sind, wird bereits deutlich, dass das Programm mit dem Dateisystem arbeitet und dabei Text verarbeitet wird, wobei das XML-Format eine Rolle spielt.\n",
    "\n",
    "Der originale Quellcode enthält eine Reihe Bibliotheken, die nach dem Defactoring nicht mehr benötigt werden. Einige entfallen durch das Defactoring, weil der Code für eine bessere Lesbarkeit umgeschrieben wurde. Dazu gehören *argparse*, *gzip*, *tarfile* und *sys*. Diese allgemeinen Bibliotheken wurden verwendet, um verschiedene Konfigurationen und Eingabeformate verarbeiten zu können. Da die Formate in diesem Anwendungsfall durch die zur Verfügung gestellten Daten feststehen, wurde an dieser Stelle auf das Anbieten von Alternativen verzichtet.  \n",
    "Außerdem wurde im Original die gesamte Bibliothek *nltk* zusätzlich zur Methode *sent_tokenize* importiert, in dieser Form jedoch nicht weiter verwendet. Im Unterschied zum Laden der gesamten Bibliothek wird mit dem Befehl *from nltk.tokenize import sent_tokenize* explizit nur die spezifische Methode *sent_tokenize* geladen. Der Vorteil davon liegt zum einen darin, dass keine unnötigen Methoden in den Speicher geladen werden. Zum anderen ist die Methode dadurch direkt über ihren Methodennamen aufrufbar, was die Lesbarkeit für Menschen vereinfacht.  \n",
    "Dass im Original zusätzlich die gesamte NLTK-Bibliothek importiert wurde, weist möglicherweise darauf hin, dass zu einem früheren Zeitpunkt mit verschiedenen Methoden der Bibliothek gearbeitet wurde. Die Versionskontrolle könnte darüber Aufschluss geben. Allerdings wird schon in der ersten nachvollziehbaren Version des Codes nur die Methode *sent_tokenize* verwendet (siehe [Github-Historie](https://github.com/weltliteratur/vossanto/commit/fecfd93c478c567bce9f69eef534c4a8db73611c#diff-6c0bf7a6645955c2ca40adbff492c77c))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konfiguration\n",
    "\n",
    "In diesem Abschnitt werden Parameter eingestellt, die im weiteren Verlauf der Analyse verwendet werden. Dazu gehören die Pfade zu verwendeten Dateien (wie dem Verzeichnis der NYT-Artikeln und der Liste mit Wikidata-Einträgen zu Personen).\n",
    "\n",
    "Die Variablen für Verzeichnisse wurden im Rahmen des Defactorings angelegt. Fischer und Jäschke arbeiten vor allem über die Kommandozeile und übergeben Werte wie Dateinamen als Argumente mit dem Aufruf eines Skripts. Dadurch ist es jedoch schwerer nachzuvollziehen, an welcher Stelle im Code die übergebene Datei konkret verarbeitet wird. Daher werden die Dateien hier direkt den betreffenden Methoden übergeben. Der besseren Lesbarkeit halber werden diese Variablen zugeordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFACTORING NAMESPACE\n",
    "\n",
    "### Verzeichnisse und Dateien\n",
    "NYT_DIRECTORY = 'nyt_corpus_1987'\n",
    "WIKIDATA_HUMANS = 'wikidata_humans_prep.tsv'\n",
    "BLACKLIST_FILE = 'blacklist_prep.tsv'\n",
    "\n",
    "### zunächst leeres Set, in dem später die Blacklist gespeichert wird\n",
    "blacklist = set()\n",
    "\n",
    "### Reguläre Ausdrücke\n",
    "\n",
    "### Muster für Vossantos\n",
    "re_theof = {\n",
    "    # 1: simple match\n",
    "    1: re.compile(\"(\\\\bthe\\\\s([A-Z][a-z]+\\\\s+){1,3}of\\\\b)\"),\n",
    "    # 2: more than three words\n",
    "    2: re.compile(\"(\\\\bthe\\\\s([A-Z][a-z]+\\\\s+)+of\\\\b)\"),\n",
    "    # 3: lower-case characters and at most five words\n",
    "    3: re.compile(\"(\\\\bthe\\\\s([A-Za-z]+\\\\s+){1,5}of\\\\b)\"),\n",
    "    # 4: all unicode characters (+non-greedy)\n",
    "    4: re.compile(\"(\\\\bthe\\\\s(\\\\w+\\\\s+){1,5}?of\\\\b)\", re.UNICODE),\n",
    "    # 5: all unicode characters and .-,'\n",
    "    5: re.compile(\"(\\\\bthe\\\\s+([\\\\w.,'-]+\\\\s+){1,5}?of\\\\b)\", re.UNICODE)\n",
    "}\n",
    "\n",
    "### Muster, das prüft, ob ein String \"the\" (umgeben von Leerzeichen) enthält\n",
    "re_the = re.compile(r\".*\\bthe\\s+(.*)$\")\n",
    "\n",
    "### Muster, das zwei aufeinander folgende Anführungszeichen identifiziert.\n",
    "### (Einträge in der Wikidata-Liste haben das muster '\"Q42\"    \"Michael Adams\"')\n",
    "re_quotes = re.compile(\"\\\"\\\"\")\n",
    "\n",
    "### Muster für Leerzeichen\n",
    "re_ws = re.compile('[\\n\\t\\r]+')\n",
    "\n",
    "### Muster für weitere Steuerzeichen\n",
    "# to remove control characters, see\n",
    "# https://stackoverflow.com/questions/92438/stripping-non-printable-characters-from-a-string-in-python\n",
    "control_chars = ''.join(map(chr, list(range(0,32)) + list(range(127,160))))\n",
    "control_char_re = re.compile('[%s]' % re.escape(control_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst werden die Dateipfade für die NYT-Artikel, die Personenliste von Wikidata und die erzeugte Blacklist in Variablen gespeichert. Für solche Werte, die sich im Laufe des Programms nicht verändern, werden Konstanten verwendet. Sie werden in Python laut [Konvention](https://www.python.org/dev/peps/pep-0008/#constants) durch Großbuchstaben gekennzeichnet. Wichtig dabei ist, dass die entsprechenden Variablen nur den Pfad zu den Dateien als String-Wert enthalten, nicht aber die Dateiinhalte selbst. Dass es sich bei einem übergebenen Wert um eine Datei handelt, muss in Python auf vorgeschriebene Art mitgeteilt werden. Dieses Vorgehen findet innerhalb später verwendeter Methoden selbst statt.\n",
    "\n",
    "Anschließend wird ein leeres Set für die Blacklist angelegt. Diesem Set wird später der Inhalt der Datei übergeben, deren Pfad in *BLACKLIST_FILE* gespeichert wurde. Die Schritte sind voneinander getrennt, um den Effekt der Blacklist durch einen Vergleich der Analyse mit und ohne Blacklist sichtbar zu machen. Auch im Original arbeiten Fischer und Jäschke mit einem zunächst leeren Set, dem bei Bedarf über die Kommandozeile eine Datei übergeben werden kann.  \n",
    "\n",
    "Die folgenden Variablen speichern verschiedene Regex-Muster, nach denen die übergebenen Dateien durchsucht werden. In der Variable *re_theof* werden verschiedene Regex-Muster gespeichert, mit denen später Vossantos in den NYT-Artikeln identifiziert werden sollen. Fischer und Jäschke haben insgesamt fünf Muster vorbereitet, mit denen die Texte analysiert werden können. Das in der Studie verwendete und standardmäßig voreingestellte Muster ist Nummer 5. Damit wird nach Textstellen gesucht, in denen die Wörter \"the\" und \"of\" im Abstand von maximal 5 Wörtern zueinander auftauchen. Da die Variable *re_theof* ein Dictionary ist, kann das gewünschte Muster jeweils durch die zugeordnete Zahl ausgewählt werden.  \n",
    "Die verschiedenen Muster sind möglicherweise das Ergebnis eines iterativen Prozesses, bei dem alternative Muster getestet und deren Erfolg verglichen wurde. Das fünfte Muster lässt im Vergleich zu den anderen die meisten Wörter und Zusatzzeichen als SOURCE zu. Dadurch, dass auch die anderen Muster Teil der Variable sind, kann bei der Analyse leicht zwischen den verschiedenen Mustern umgeschalten werden. Die Kommentare über jedem einzelnen Muster deuten darauf hin, dass die Muster selbst, die der Methode *re.compile* in Klammern übergeben werden, für Menschen nicht selbsterklärend sind.  \n",
    "Die beiden Variablen *re_ws* und *control_char_re* enthalten jeweils Muster, mit denen nach Steuerzeichen gesucht wird. Steuerzeichen haben für Menschen keine unmittelbare Bedeutung. Sie geben Maschinen Auskunft über die Art der Darstellung und helfen insofern dabei, Text für Menschen verständlich zu strukturieren (Beispiele sind z. B. Tabs oder Zeilenendezeichen). Da die NYT-Artikel mithilfe von *re_theof* nach festen Zeichenfolgen durchsucht werden, ist es wichtig, nicht relevante Zeichen vorher auszusortieren. Andernfalls könnten Textstellen aussortiert werden, die von Menschen betrachtet als Treffer gezählt werden würden, da sie die Steuerzeichen nicht wahrnehmen.   \n",
    "\n",
    "An dieser Stelle lohnt sich außerdem die Betrachtung der Variablenbenennung. Dabei fällt auf, dass alle Variablen, denen ein durch die Methode *re.compile* erzeugter Wert zugeordnet wird, das Kürzel \"re\" enthalten. Dadurch wird eine Ähnlichkeit zwischen den Variablen verdeutlicht und ein direkter Bezug zur Bibliothek *re* hergestellt, den der Computer selbst nicht benötigen würde. Für Menschen ist dadurch nachvollziehbar, dass die Variablen Objekte mit dem Datentyp *Pattern* enthalten (siehe [Dokumentation](https://docs.python.org/3/library/re.html#re-objects)). Mit Ausnahme der letzten Variable steht das Kürzel außerdem an vorderster Stelle im Namen und wird von einer Beschreibung des gesuchten Musters gefolgt. Dieses Vorgehen zur Verknüpfung von Datentyp und Benennung wird noch an anderer Stelle verwendet (siehe Kapitel *Datenaufbereitung*).  \n",
    "Die Benennung von *control_char_re* ist vermutlich durch den im Kommentar darüber verlinkten Beitrag auf Stack Overflow beeinflusst. Der Kommentar richtet sich mit expliziter Anrede an zukünftige Lesende und legt offen, woher der Ansatz zur Identifizierung von Steuerungszeichen stammt. Damit wird deutlich, dass der Code gezielt aufbereitet wurde, um möglichst verständlich zu sein. Neben der Formulierung von Kommentaren wie diesem deuten auch die umfangreichen Beschreibungen und zusätzlichen Informationen zur [Durchführung der Analyse](https://github.com/weltliteratur/vossanto/blob/master/theof/method.org) darauf hin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenaufbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Abschnitt werden die XML-Dateien der NYT-Artikel eingelesen und so aufbereitet, dass sie später durchsucht werden können. Um die einzelnen Schritte bei der Aufbereitung nachvollziehen zu können, werden immer wieder exemplarische Ansichten (Inspektionen) des Zustands der Texte gezeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Korpus der NYT-Artikel enthält für das Jahr 1987 insgesamt 106104 Texte.\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### Überblick über das Verzeichnis mit den NYT-Artikeln\n",
    "\n",
    "file_amount = len([file for file in os.listdir(NYT_DIRECTORY)])\n",
    "print(f'Das Korpus der NYT-Artikel enthält für das Jahr 1987 insgesamt {file_amount} Texte.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insgesamt kann das Defactoring mit 106.104 Texten durchgeführt werden. Im Vergleich zur Originalstudie, die 1.8 Millionen Artikel der Jahre von 1987-2007 analysiert hat, ist diese Zahl gering. Die Analyse lässt sich dennoch in Teilen reproduzieren. Der Code wurde so geschrieben, dass er mit einer beliebigen Artikelzahl durchgeführt werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION\n",
    "### Einlesen der Dateiliste\n",
    "\n",
    "def gen_files(path):\n",
    "    for fname in os.listdir(path):\n",
    "        fname = path + \"/\" + fname\n",
    "        if os.path.isfile(fname):\n",
    "            yield open(fname, \"rt\"), fname\n",
    "            \n",
    "### DEFACTORING NAMESPACE\n",
    "files = gen_files(NYT_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Methode *gen_files* werden die XML-Dateien aus dem Dateisystem eingelesen. Sie werden mit dem Methodenaufruf in der Variable *files* gespeichert. Gegenüber dem Originalcode wurde die Methode beim Defactoring deutlich verkürzt. Im Original werden if-else-Abfragen verwendet, um zu prüfen, ob es sich bei der übergebenen Datei um eine einzelne XML-Datei oder ein normales oder komprimiertes Verzeichnis handelt. Weil hier mit relativ großen Datenmengen gearbeitet wird, ist es sinnvoll, die Durchführung sowohl mit einzelnen Dateien als auch mit komprimierten Verzeichnissen zu ermöglichen. So kann das Programm beispielsweise zu Testzwecken mit einer einzelnen Datei gestartet werden, ohne dass eine Anpassung im Code notwendig ist. Im konkreten Fall wird mit einem nicht komprimierten Verzeichnis gearbeitet, weshalb die zusätzlichen Abfragen nicht benötigt werden.  \n",
    "\n",
    "Der Rückgabewert der Methode wird durch den Ausdruck *yield* geliefert. Damit wird in Python ein *Generatorobjekt* (siehe [Dokumentation](https://www.python.org/dev/peps/pep-0289/)) erzeugt. Ein Generatorobjekt ist ein spezielles Iteratorobjekt und verhält sich auf den ersten Blick ähnlich wie eine Liste. So können die Elemente eines Generators mit einer Schleife iteriert werden. Anders als eine Liste wird mit *yield* jedoch immer nur das gerade benötigte Objekt des Generators zurückgeliefert und der aktuelle Zähler im Iterator festgehalten. Mit dem nächsten Aufruf des Generators wandert dieser interne Zähler weiter und liefert das nächste Objekt zurück. Dadurch muss nicht auf die Erzeugung und das Laden aller Objekte der Liste in den Zwischenspeicher gewartet werden, bevor das gerade benötigte Objekt weiter verwendet werden kann. Die Arbeit mit Generatoren eignet sich daher besonders für große Datensätze und Pipelines, die diese in mehreren Stufen verarbeiten. Der Vorteil in diesem Vorgehen liegt vor allem in einer erhöhten Performance.  \n",
    "Das Nachvollziehen von Zwischenzuständen solcher Verarbeitungspipelines wird durch Generatoren jedoch erschwert. Da jeder Aufruf des Generators den Wert des internen Zählers verändert, kann während der Ausführung des Programms nicht einfach darauf zugegriffen werden ohne das Programm zu beeinflussen. Mit der Verwendung von Generatoren wird daher eine Entscheidung für Effizienz zulasten der Verfügbarkeit von einsehbaren Zwischenergebnissen getroffen. Aufgrund der Größe der verarbeiteten Daten ist diese Entscheidung durchaus nachvollziehbar. Darüber hinaus entspricht dieses Vorgehen dem in den Digital Humanities beobachteten Fokus auf die Ergebnisse von Forschung (vgl. Andrews 2013 und Ramsay 2011).  \n",
    "Die Methodenbenennung folgt hierbei dem Schema der Variablenbenennung für reguläre Ausdrücke. Hier wird der Rückgabewert der Methode durch den Methodennamen verdeutlicht. Entsprechend beginnen alle weiteren Methoden des Programms, die ein Generatorobjekt erzeugen, mit dem Kürzel \"gen\". Anders als bei statisch typisierten Programmiersprachen wie Java muss der Rückgabewert einer Methode in Python nicht bei deren Definition festgelegt werden. Daher dient diese Information nur der Verständlichkeit für Menschen. Durch die Benennung wird außerdem ein Zusammenhang zwischen den wichtigsten Methoden der Textverarbeitung in dem Programm hergestellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für ein bessers Verständnis der folgenden Schritte, in denen die eingelesenen XML-Dateien verarbeitet werden, soll zunächst ein Überblick über deren Aufbau gegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dateipfad zur ersten Datei im Verzeichnis der NYT-Artikel: nyt_corpus_1987/0027572.xml\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "\n",
    "### Überblick über Dateipfad zur XML-Datei eines Artikels\n",
    "filepath = NYT_DIRECTORY + '/' + os.listdir(NYT_DIRECTORY)[0]\n",
    "print(f'Dateipfad zur ersten Datei im Verzeichnis der NYT-Artikel: {filepath}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<!DOCTYPE nitf SYSTEM \"http://www.nitf.org/IPTC/NITF/3.3/specification/dtd/nitf-3-3.dtd\">\n",
      "<nitf change.date=\"June 10, 2005\" change.time=\"19:30\" version=\"-//IPTC//DTD NITF 3.3//EN\">\n",
      "  <head>\n",
      "    <title>HERO OF THE LIFE CYCLE</title>\n",
      "    <meta content=\"5\" name=\"publication_day_of_month\"/>\n",
      "    <meta content=\"4\" name=\"publication_month\"/>\n",
      "    <meta content=\"1987\" name=\"publication_year\"/>\n",
      "    <meta content=\"Sunday\" name=\"publication_day_of_week\"/>\n",
      "    <meta content=\"Book Review Desk\" name=\"dsk\"/>\n",
      "    <meta content=\"36\" name=\"print_page_number\"/>\n",
      "    <meta content=\"7\" name=\"print_section\"/>\n",
      "    <meta content=\"2\" name=\"print_column\"/>\n",
      "    <meta content=\"Arts; Books\" name=\"online_sections\"/>\n",
      "    <docdata>\n",
      "      <doc-id id-string=\"27572\"/>\n",
      "      <doc.copyright holder=\"The New York Times\" year=\"1987\"/>\n",
      "      <series series.name=\"MIND/BODY/HEALTH\"/>\n",
      "      <identified-content>\n",
      "        <classifier class=\"indexing_service\" type=\"descriptor\">BOOK REVIEWS</classifier>\n",
      "        <person class=\"indexing_service\">TURKLE, SHERRY</person>\n",
      "        <person class=\"indexing_service\">ERIKSON, ERIK H</person>\n",
      "        <object.title class=\"indexing_service\">WAY OF LOOKING AT THINGS, A (BOOK)</object.title>\n",
      "        <classifier class=\"online_producer\" type=\"types_of_material\">Review</classifier>\n",
      "        <classifier class=\"online_producer\" type=\"taxonomic_classifier\">Top/Features/Books/Book Reviews</classifier>\n",
      "        <classifier class=\"online_producer\" type=\"taxonomic_classifier\">Top/Features/Arts</classifier>\n",
      "        <classifier class=\"online_producer\" type=\"taxonomic_classifier\">Top/Features/Books</classifier>\n",
      "        <classifier class=\"online_producer\" type=\"general_descriptor\">Books and Literature</classifier>\n",
      "      </identified-content>\n",
      "    </docdata>\n",
      "    <pubdata date.publication=\"19870405T000000\" ex-ref=\"http://query.nytimes.com/gst/fullpage.html?res=940DE4DB1131F936A35757C0A961948260\" item-length=\"1620\" name=\"The New York Times\" unit-of-measure=\"word\"/>\n",
      "  </head>\n",
      "  <body>\n",
      "    <body.head>\n",
      "      <hedline>\n",
      "        <hl1>HERO OF THE LIFE CYCLE</hl1>\n",
      "      </hedline>\n",
      "      <byline class=\"print_byline\">By SHERRY TURTLE; Sherry Turkle, an associate professor of sociology at the Massachusetts Institute of Technology, is the author of ''Psychoanalytic Politics: Freud's French Revolution'' and ''The Second Self: Computers and the Human Spirit.''</byline>\n",
      "      <byline class=\"normalized_byline\">TURKLE, SHERRY</byline>\n",
      "    </body.head>\n",
      "    <body.content>\n",
      "      <block class=\"lead_paragraph\">\n",
      "        <p>LEAD:  A WAY OF LOOKING AT THINGS  Selected Papers From 1930 to 1980. By Erik H. Erikson. Edited by Stephen Schlein. 782 pp. New York: W. W. Norton &amp; Company. $29.95.</p>\n",
      "      </block>\n",
      "      <block class=\"full_text\">\n",
      "        <p>LEAD:  A WAY OF LOOKING AT THINGS  Selected Papers From 1930 to 1980. By Erik H. Erikson. Edited by Stephen Schlein. 782 pp. New York: W. W. Norton &amp; Company. $29.95.</p>\n",
      "        <p>A WAY OF LOOKING AT THINGS  Selected Papers From 1930 to 1980. By Erik H. Erikson. Edited by Stephen Schlein. 782 pp. New York: W. W. Norton &amp; Company. $29.95.</p>\n",
      "        <p>AMERICA is a psychoanalytic culture. In this country, Freudian ideas have been taken up not only as the basis for a therapeutic technique, but as the foundation for a more widespread sensibility. Since the publication of ''Childhood and Society'' in 1950, Erik Erikson has become emblematic of that sensibility.</p>\n",
      "        <p>Mr. Erikson once described psychoanalysis as ''the principal modern form of systematic introspection and meditation.'' In fact, the language he invented - of ''identity,'' ''identity crisis'' and ''the life cycle'' - has become the principal American form of thinking about adolescence and, beyond this, about the widest range of adult trials and tribulations.</p>\n",
      "        <p>The publication of ''A Way of Looking at Things,'' a volume of Mr. Erikson's previously uncollected papers edited by Stephen Schlein, a clinical psychologist and a teacher at the Cambridge Hospital of Harvard Medical School, offers a fresh occasion to reflect on why. Spanning 50 years of creative work, the papers cover Mr. Erikson's major preoccupations. Some of these will be well known to most readers: history, child development and identity. Other themes are less familiar. The collection includes classic papers that never found the audience they deserved - in particular, a set of essays that weave play with dreams and dreams with life history.</p>\n",
      "        <p>In ''Configurations in Play'' and ''Studies in the Interpretation of Play,'' Mr. Erikson develops the highly original technique of interpreting children's play sequences the way an analyst interprets a dream. In observing the process of play, he looks for cadence, structure, disruption and spatial configuration as clues to its meaning. In ''The Dream Specimen of Psychoanalysis,'' Mr. Erikson uses his idea of the life cycle to throw new light on Freud's most famous dream. This is the ''Irma dream,'' a dream that Freud had about one of his own patients, and in his own words, ''the first dream ever subjected to an exhaustive interpretation.'' Mr. Erikson uses elements of Freud's biography, in particular his letters to Wilhelm Fliess, to put the Irma dream in the context of ''the moment in Freud's life when it was dreamed . . . the moment when creative thought gave birth to the interpretation of dreams.'' In doing so, Mr. Erikson finds more than the displaced conflicts of Freud's past. He finds Freud's aspirations for his future, going so far as to suggest that the Irma dream may carry ''the historical burden of being dreamed in order to be analyzed, and analyzed in order to fulfill a very special fate.''</p>\n",
      "        <p>The questions that these essays pose for the reader in 1987 are not only what was original and powerful in what Mr. Erikson wrote, but what made it easy for people - particularly Americans - to take up what he had to say. What stands behind the special love affair between Mr. Erikson and America?</p>\n",
      "        <p>The collection as a whole offers a view of Mr. Erikson as a modern-day Tocqueville. In these essays, he uses his psychoanalytic sensibility to touch base with 50 years of American cultural life (there are papers on American Indian tribes, American youth, women and blacks) and with 50 years of American political history. We read about his refusal to sign a loyalty oath during the McCarthy era, about student protest during the Vietnam era and about the American landing on the moon. Mr. Erikson had fallen in love with America and Americans responded, not only because he wrote so movingly about them, but because of a fundamental resonance between his philosophy and American dreams.</p>\n",
      "        <p>Whereas the orthodox Freudian ''stages'' -oral, anal, phallic - seemed pessimistic, and were popularized in the adage that, ''All is decided by the age of 5,'' Mr. Erikson's work - with its emphasis on adult development and on a life cycle where early themes get a chance to be replayed at later stages - suggested that we all get many chances. Americans wanted to hear that optimistic message. It fit in with what they had made of psychoanalysis, with the ''therapeutic culture'' they had built.</p>\n",
      "        <p>Freud devoted most of his career to the study of the instincts, but toward the end of his life he turned his attention to the ego, facing out toward reality. Ego psychology began in Vienna, but it was in America that it found a spiritual home. American ego psychology was directed toward an active adaptation of the patient to reality - toward what came to be called ''coping.'' It brought Freudianism in line with American beliefs about the virtue and necessity of a positive approach.</p>\n",
      "        <p>There was a good fit between this therapeutic culture and Mr. Erikson's portrait of a life cycle that gives ''second chances'' for opportunities missed and paths not taken, just as there was a good fit between the American personality that the sociologist David Riesman called ''other directed'' and Mr. Erikson's portrait of an adolescence exquisitely permeable to outside influence. Mr. Erikson's emphasis on the formative crisis of adolescence also captured the American imagination: America is after all a nation that worships its young. AMERICAN individualism represents each person as a virtuoso of his or her self.</p>\n",
      "        <p>Americans see themselves as making their fate (and remaking themselves) by changing their social situation. Here, too, there was a congruence between American ideals and the message of Mr. Erikson, an immigrant who had twice remade his fate, first in Vienna and then in the United States.</p>\n",
      "        <p>Erik Erikson, who was born in Germany in 1902, came to Vienna in 1927 as an itinerant artist with only a high school diploma. He left in 1933 as a psychoanalyst, to settle permanently in the United States.</p>\n",
      "        <p>His theory of ''great men'' like Freud, Luther and Gandhi, is based on the idea of a fit between their personal crises and the crises of their times. Their solutions - in the form of their ideas - become cultural solutions. Out of Mr. Erikson's own situation of marginality - as an artist, a foreigner, a man without a college education in a profession that is usually open only to physicians - he lived out, resolved and theorized a crisis of identity that spoke to the hearts of Americans who so often feel displaced, uprooted, betwixt and between.</p>\n",
      "        <p>Implicit in Freud's work there is another theory of why certain psychological ideas capture the popular imagination. People like Freudian interpretations because they offer a way to come closer to aspects of ourselves, like sexuality and aggression, which we censor but want to be in contact with. Freud's theories are ''evocative.'' For example, armed with the idea of the ''significant'' slip of the tongue, you can play with your own mistakes (and those of your friends), you can actively try to figure out new meanings. Slips are almost-tangible ideas; they are ''objects-to-think-with.''</p>\n",
      "        <p>Mr. Erikson's theories, like Freud's, have qualities that allow people to take them up and make them their own: his ideas are evocative, putting us in touch with what disturbs, and they are also tangi-ble. For example, he has insisted on the body as a template for the psyche. His research with children showed that boys tend to use blocks to build towers while girls build enclosed and protected spaces, reflecting their respective understandings of their bodies. The idea is easy to appropriate: simple and evocative, powerful and manipulable. You can watch children. You can watch yourself - as you doodle on the margins of a notebook, or build sand castles at the beach.</p>\n",
      "        <p>A Harvard University student in the 1960's went so far as to write a paper on the sexual integration of Lamont, a university library which had just been opened to women. He took photographs of the wooden desks of Lamont before and after women's arrival. The ''before'' desks had been scratched with pencil marks of nervous or bored male students - there were arrows, darts and rockets. The photographs of the ''after'' desks showed these same projectiles, now encircled by flowers and curving laced patterns. The student glowed with triumph: he had proved Mr. Erikson right.</p>\n",
      "        <p>What is important here is not the research project that illustrated what Mr. Erikson had already demonstrated with far more rigor and grace, but the sense of satisfaction the student felt from the re-demonstration. We want to be in touch with our sense of our bodies; Mr. Erikson's theory offers a way that is concrete and poses little threat.</p>\n",
      "        <p>The phrase ''a way of looking at things'' was originally Freud's. Freud was cautious about ego-psychology, fearful that it might compromise what he considered the core of psychoanalysis. And so he warned that in the exploration of the ego, ''it will be difficult to escape what is universally known; it will rather be a question of new ways of looking at things.'' At the end of ''Childhood and Society,'' Mr. Erikson says that he has ''nothing to offer except a way of looking at things.'' But the valence of Freud's phrase has been reversed. What is most powerful about reading Mr. Erikson is the experience of recognition. But this does not mean that he merely restates what is ''universally known.'' He tells us what we knew but did not or could not say -and that turns out to be a revolution in vision.</p>\n",
      "        <p>''A Way of Looking at Things'' allows us to watch Mr. Erikson as he developed ideas that were to become cornerstones of American thinking about the self. His language is so much in our culture that many people feel they ''know'' him without having to read him. Perhaps the greatest contribution of this book is that it will send us back to Erik Erikson.</p>\n",
      "        <p>MIND/BODY/HEALTH</p>\n",
      "      </block>\n",
      "    </body.content>\n",
      "  </body>\n",
      "</nitf>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### Beispielausgabe einer XML-Datei\n",
    "\n",
    "with open(filepath, 'r') as example_file:\n",
    "    print(example_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe einer Beispieldatei des NYT-Korpus zeigt, dass diese zusätzlich zum Text des Artikels eine Reihe Metaangaben enthält. Der Artikelinhalt befindet sich im XML-Tag *`<block class=\"full_text\">`*. In den folgenden Schritten der Auswertung werden diese Auszeichnungen durchsucht und in Textform gespeichert. Fischer und Jäschke speichern zunächst alle Meta-Angaben und den Artikelinhalt in Form von eigenen Variablen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml2str(f):\n",
    "    tree = ET.parse(f)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # path to the heading\n",
    "    #\n",
    "    # ./body/body.head/hedline vs. ./body/body.head/\n",
    "    # ./body/body.head/ and ./body/body.head/hedline include only the\n",
    "    # heading itself, not the first introductory sentence which we get\n",
    "    # with ./body/body.head\n",
    "    heading = root.findall(\"./body/body.head\")\n",
    "    head = \"\"\n",
    "    for block in heading:\n",
    "        head += ET.tostring(block, encoding=\"utf-8\", method=\"text\").decode(\"utf-8\")\n",
    "\n",
    "    # path to the main text block\n",
    "    content = root.findall(\"./body/body.content/\")\n",
    "    body = \"\"\n",
    "    for block in content:\n",
    "        if block.attrib[\"class\"] == \"full_text\":\n",
    "            # strip XML\n",
    "            body += \"\\n\" + ET.tostring(block, encoding=\"utf-8\", method=\"text\").decode(\"utf-8\")\n",
    "    return head + \"\\n\\n\" + body\n",
    "\n",
    "def gen_text(files):\n",
    "    for f, fname in files:\n",
    "        yield fname, xml2str(f)\n",
    "\n",
    "### DEFACTORING NAMESPACE\n",
    "### Erzeugen einer Liste aus Texten\n",
    "texts = gen_text(files)\n",
    "\n",
    "### Aufteilung der Texte in Sätze\n",
    "def gen_sentences(texts):\n",
    "    for fname, text in texts:\n",
    "        # split at line breaks, since the XML preserved paragraphs\n",
    "        for line in text.splitlines():\n",
    "            for sentence in sent_tokenize(line):\n",
    "                yield fname, sentence\n",
    "\n",
    "### DEFACTORING NAMESPACE\n",
    "sents = gen_sentences(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An dieser Stelle geht die Verarbeitungspipeline der NYT-Artikel mit Generatorobjekten weiter. Die Methode *xml2str* ist eine Hilfsmethode, die von *gen_text* verwendet wird. Sie verwendet das zu Beginn importierte XML-Modul, mit dem die XML-Dateien geparst werden können. Dabei werden zunächst sämtliche Metadaten zum Artikel gespeichert. Anschließend wird der Artikelinhalt gesucht und hinzugefügt. Die Kommentare in der Methode zeigen die Pfade zu den Metaangaben (./body/body.head) bzw. dem Artikel (./body/body.content) an. Die Methode *gen_files* verwendet *xml2str* und speichert den Dateiinhalt jeweils zusammen mit dem dazugehörigen Dateinamen.  \n",
    "Anschließend werden die Texte mit der Methode *gen_sentences* in Sätze eingeteilt. Auch hier wird für jeden Satz der dazugehörige Dateiname gespeichert, um die Zuordnung zum entsprechenden Artikel zu gewährleisten. Für die Aufteilung der Texte in Sätze wird die NLTK-Methode *sent_tokenize* verwendet. Mit diesem Schritt sind die Artikel in sehr kleine Einheiten aufgeteilt und aufbereitet, damit sie anschließend nach Vossantos durchsucht werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auswertung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Auswertung erfolgt in mehreren Schritten, bei denen die Regeln für mögliche Vossantos immer weiter verschärft werden. Zuerst werden die Texte nach dem Muster *\"the SOURCE of\"* durchsucht. Anschließend wird die Trefferliste mit der Personenliste aus dem Datensatz von Wikidata abgeglichen. Als letztes wird die Blacklist hinzugezogen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 1: Regex-Muster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_regex(texts, regex, groupid=0):\n",
    "    for fname, text in texts:\n",
    "        for m in regex.finditer(text):\n",
    "            # also return the start position of the matches\n",
    "            yield fname, m.group(groupid), m.start(groupid), text\n",
    "            \n",
    "def gen_rm_ctrl(texts):\n",
    "    for cols in texts:\n",
    "        #yield [re_ws.sub(' ', col) for col in cols]\n",
    "        res = []\n",
    "        for col in cols:\n",
    "            if isinstance(col, int):\n",
    "                res.append(col)\n",
    "            else:\n",
    "                res.append(re_ws.sub(' ', col))\n",
    "        yield res\n",
    "\n",
    "### DEFACTORING NAMESPACE        \n",
    "### Anwendung des Regex-Musters\n",
    "match = gen_regex(sents, re_theof[5])\n",
    "match = gen_rm_ctrl(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im ersten Schritt der Auswertung wird weiter mit Generatorobjekten gearbeitet. Die Methode *gen_regex* durchsucht die zuvor eingeteilten Sätze der NYT-Artikel (die in der Variable *sents* gespeichert wurden) nach dem übergebenen Regex-Muster, welches mit *re_theof[5]* standardmäßig voreingestellt ist. Durch Anpassung der Zahl kann hier auf die verschiedenen, in *re_theof* gespeicherten, Muster zurückgegriffen werden. Standardmäßig werden alle Sätze, die dem Muster \"the SOURCE of\" entsprechen, in der Trefferliste *match* gespeichert. Anschließend werden die Treffer um die in *re_ws* gespeicherten Steuerzeuchen wie Tabs und Zeilenendezeichen mit der Methode *gen_rm_ctrl* aus der Trefferliste entfernt.  \n",
    "Bei der Methode fällt auf, dass darin die zweite Zeile Code auskommentiert ist. In der einen Zeile werden bis auf einen Unterschied die gleichen Schritte durchgeführt wie in den darauf folgenden Zeilen der Methode. Allerdings fehlt dabei die *if-else* Kontrollstruktur. Diese prüft durch die Methode *isinstance*, ob die Werte der Spalten Zahlenwerte enthalten. Nur bei Werten, die keine Zahlen sind, wird die Bereinigung um Leerzeichen durchgeführt. Da diese Kontrolle in der auskommentierten Zeile fehlt, würde dort auch bei Zahlen versucht, Leerzeichen zu entfernen. Diese Operation ist jedoch semantisch falsch, daher würde das Programm einen Fehler werfen und abbrechen.  \n",
    "\n",
    "Anders als beim sonstigen Vorgehen werden hier die Ergebnisse zwei verschiedener Methoden in der selben Variable gespeichert. Sowohl die Trefferliste der Vossanto-Muster als auch deren Bereinigung um Steuerzeichen werden in *match* gespeichert, wodurch die unbereinigte Trefferliste überschrieben wird. Sie steht damit nicht mehr für weitere Berechnungen zur Verfügung. Im Gegensatz dazu wurden bisher alle Zwischenergebnisse der Textverarbeitung in eigenen Variablen gespeichert. Diese Abweichung lässt sich möglicherweise dadurch erklären, dass der Unterschied in der Trefferliste mit und ohne Steuerzeichen für Menschen keine Bedeutung spielt – er ist nur für die Behandlung der Texte durch den Computer relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GROßER VERARBEITUNGSAUFWAND###\n",
    "### DEFACTORING FUNCTION\n",
    "\n",
    "def print_matches(matches, output_file, sep='\\t'):\n",
    "    for fname, match, index, text in matches:\n",
    "        print(fname, match, text, file=output_file, sep=sep)\n",
    "\n",
    "\n",
    "with open('theof_1987.tsv', 'w') as output:\n",
    "    print_matches(match, output, '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dieser Zelle endet die Verarbeitungspipeline mit Generatoren. Die Methode *print_matches* gibt die Trefferliste möglicher Vossantos als Tabelle aus. Die Methode wurde gegenüber dem Original soweit angepasst, dass die Ausgabedatei schon in der Methode übergeben wird. Im Original wird die Ausgabe der Ergebnisse über die Kommandozeile geregelt, weshalb der Eingabeparameter *output_file* dort nicht existiert.  \n",
    "Das print-Statement der Methode gibt Auskunft über den Aufbau der Tabelle: Für jeden Treffer werden darin der Dateiname, das identifizierte Muster, sowie der vollständige Satz, in dem der Treffer gefunden wurde, ausgegeben.  \n",
    "Mit der for-Schleife werden alle Objekte des Generators abgefragt und in die Tabelle geschrieben. Wegen der Größe des Datensatzes ist diese Operation relativ aufwendig und kann, je nach System auf dem sie ausgeführt wird, lange dauern. Daher muss sie nicht zwingend ausgeführt werden. Der folgende Code arbeitet mit einer vorbereiteten Datei weiter (die durch die Endung \"_prep\" im Dateinamen gekennzeichnet ist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641432 theof_1987_prep.tsv\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### Übersicht über die häufigsten Treffer\n",
    "!wc -l theof_1987_prep.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Veranschaulichung von Zwischenschritten setzen Fischer und Jäschke auf Shellbefehle. Diese Art von Befehlen wird auch in diesem Notebook verwendet um Zwischenzustände der Analyse genauer zu betrachten. Hier werden durch das Programm *wc* die Zeilen der erzeugten Datei gezählt. Dadurch lässt sich überprüfen, wie viele Treffer durch den ersten Ansatz zur Identifizierung der Vossantos gefunden wurden.\n",
    "\n",
    "Insgesamt wurden 641.432 potenzielle Treffer für Vossantos gefunden. Die folgende Zelle gibt einen Überblick über deren Inhalt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7102 the end of\r\n",
      "   6606 the University of\r\n",
      "   4287 the number of\r\n",
      "   3206 the sale of\r\n",
      "   3172 the rest of\r\n",
      "   2370 the use of\r\n",
      "   2172 the kind of\r\n",
      "   2135 the son of\r\n",
      "   1943 the age of\r\n",
      "   1883 the president of\r\n",
      "sort: Schreiben fehlgeschlagen: Standardausgabe: Datenübergabe unterbrochen (broken pipe)\r\n",
      "sort: Schreibfehler\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### Überblick über die Datei\n",
    "\n",
    "!awk -F'\\t' '{print $2}' theof_1987_prep.tsv| sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe zeigt die 10 häufigsten Treffer möglicher Vossanto im NYT-Datensatz und deren Häufigkeit. Diese werden in erster Linie durch das Programm *awk* (siehe [Dokumentation](http://www.gnu.org/software/gawk/manual/html_node/index.html)) ermittelt. Awk liest die Tabelle der Vossantotreffer ein und gibt durch *{print $2}* jeweils die Werte der zweiten Spalte zurück. Darin sind die potenziellen Vossantos gespeichert. Durch *sort* werden diese alphabetisch sortiert. Mit *uniq -c* wird gezählt, wie häufig einzelne Treffer vorkommen. Diese werden anschließend mit *sort -nr* nochmals nach der Häufigkeit des Aufkommens sortiert. Die Fehlermeldung am Ende der Tabelle wird durch *sort* verursacht, da das Programm alle Ergebnisse ausgeben soll. Mit dem Kommando *head* wird die Ausgabe jedoch nach den ersten 10 Treffern beendet, weshalb *sort* seine Aufgabe nicht vollständig erfüllen kann.  \n",
    "\n",
    "Die Ausgabe zeigt, dass die Suche nach Vossantos allein auf Basis vorgegebener Zeichen wenig bei der Identifizierung von Vossantos hilft. Keiner der 10 häufigsten Treffer enthält den Namen einer Person. Um diese *false positives* auszusortieren, wird im nächsten Schritt die Personenliste von Wikidata hinzugezogen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 2: Regex-Muster und Wikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikidata-Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst werden mithilfe des [Wikidata Toolkit](https://www.mediawiki.org/wiki/Wikidata_Toolkit) sämtliche Datenbankeinträge von Wikidata heruntergeladen. Das Wikidata Toolkit ist eine Java-Bibliothek. Sie stellt Klassen und Methoden zur Verarbeitung von Daten aus Wikidata bereit. Mit diesen kann der Datendump nach dem Download nach allen Einträgen durchsucht werden, die zu einer Person gehören. Diese Art der Abfrage ist möglich, da Wikidata strukturierte Daten in Form von *Linked Open Data* enthält. Alle Einträge (*Items*) werden durch bestimmte Aussagen beschrieben. Diese Aussagen bestehen aus Attributen (*properties*) und Werten (*values*). Konkret wird bei der Abfrage nach allen Einträgen gefragt, die das Attribut *P31* (entspricht *instanceOf*) mit dem Wert *Q5* (entspricht *human*) haben.  \n",
    "Fischer und Jäschke haben zwar den Code zur Erzeugung der Personenliste veröffentlicht. Dieser Schritt der Auswertung lässt sich dennoch nicht reproduzieren, da der originale Datensatz von Wikidata nicht rekonstruiert werden kann. Wikidata wird ständig bearbeitet und vollständige Zustände der Datenbank werden nur für etwas mehr als einen Monat gespeichert. Zum Vergleich: Die Personenliste von Fischer und Jäschke wurde vor dem 19.07.2017 erzeugt (siehe [commit](https://github.com/weltliteratur/vossanto/commit/3a1e3f179c35807b0f2474a729ae8fae0140ec37) im Github Repositorium). Zu diesem Zeitpunkt enthielt Wikidata etwa 29 Millionen Einträge (siehe [Wikidata-Statistik](https://tools.wmflabs.org/wikidata-todo/stats.php)). Für Dezember 2019 zeigt die Statistik etwa 70 Millionen Datenbankeinträge. Da die Einträge keine Information über ihr Erstelldatum enthalten, ist ohne einen Vergleich der beiden Datensätze nicht nachvollziehbar, welche Einträge im Vergleich zu Juli 2017 hinzugekommen sind.  \n",
    "\n",
    "Auch ohne Ausführung bietet der Code durchaus einen Mehrwert für das Verständnis des Umgangs mit der Schnittstelle von Wikidata. Darüber hinaus besteht eine Abhängigkeit der hier verwendeten Personenliste vom Code. Da er für die Reproduzierung der Studienergebnisse jedoch nicht ausgeführt werden kann, wird er in diesem Notebook nicht näher berücksichtigt. Hier muss zwischen einer möglichst umfassender Betrachtung und einem Fokus auf für den Kontext relevanten Code abgewogen werden. Schmidt schreibt dazu: \"The underlying complexity of computers makes some degree of ignorance unavoidable.\" (2016). Der Code hat auf die Ausführung des Programms im Rahmen des Defactorings keine direkte Auswirkung und seine Ausführung lässt sich nicht unmittelbar nachvollziehen. Aus diesem Grund wird an dieser Stelle mit der von Fischer und Jäschke bereitgestellten Personenliste weiter gearbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2801931 wikidata_humans_prep.tsv\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### Überblick über die Wikidata-Personenliste\n",
    "!wc -l wikidata_humans_prep.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Inspektion zeigt, dass die Personenliste insgesamt 2.801.931 Einträge enthält. Diese Zahl entspricht allen Einträgen von Personen in Wikidata zum Zeitpunkt des Downloads von Fischer und Jäschke, die ein Label (also einen Namen) haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Q23\"\"George Washington\"\"Father of the United States\"\"Washington\"\"President Washington\"\r\n",
      "\"Q42\"\"Douglas Adams\"\"Douglas Noël Adams\"\"Douglas Noel Adams\"\"Douglas N. Adams\"\r\n",
      "\"Q76\"\"Barack Obama\"\"Barack Hussein Obama II\"\"Barack Obama II\"\"Barack Hussein Obama\"\r\n",
      "\"Q80\"\"Tim Berners-Lee\"\"TimBL\"\"Sir Tim Berners-Lee\"\"Timothy John Berners-Lee\"\r\n",
      "\"Q91\"\"Abraham Lincoln\"\"Abe Lincoln\"\"Lincoln\"\"Honest Abe\"\r\n",
      "\"Q157\"\"François Hollande\"\"François Gérard Georges Nicolas Hollande\"\"Francois Hollande\"\"Hollande\"\r\n",
      "\"Q181\"\"Jimmy Wales\"\"Jimbo Wales\"\"Jimmy Donal Wales\"\"Jimbo\"\r\n",
      "\"Q185\"\"Larry Sanger\"\"Lawrence Mark Sanger\"\"Lawrence Mark \"\"Larry\"\" Sanger\"\r\n",
      "\"Q186\"\"Ken Jennings\"\"Kenneth Wayne \"\"Ken\"\" Jennings III\"\r\n",
      "\"Q192\"\"David Cameron\"\"David William Donald Cameron\"\r\n",
      "awk: write failure (Broken pipe)\r\n",
      "awk: close failed on file /dev/stdout (Broken pipe)\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### Überblick über Inhalt der Liste\n",
    "!awk -F'\\t' '{print $1 $2 $3 $4 $5}' wikidata_humans_prep.tsv | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Überblick verdeutlicht den Aufbau der Tabelle: Jede Person hat eine eindeutige ID, die jeweils mit \"Q\" beginnt und von einer Zahl gefolgt wird. Durch diese ID sind die Einträge in Wikidata eindeutig identifizierbar. Auf die ID folgt der Personenname. Anschließend werden Aliase oder Synonyme für die Person aufgelistet. Diese Zuordnung verschiedener Benennungen ist für die Vossanto-Analyse besonders hilfreich. So können auch verschiedene Bezeichnungen der selben Person als solche erkannt und einander zugeordnet werden.\n",
    "\n",
    "Die Fehlermeldung zum Ende des Programms wird auch hier wieder durch das Kommando *head* versursacht, welches *awk* daran hindert, die Ausgabe aller Zeilen durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFACTORING NAMESPACE\n",
    "\n",
    "def remove_control_chars(s):\n",
    "    return control_char_re.sub('', s)\n",
    "\n",
    "def clean(s):\n",
    "    s = s[1:-1]\n",
    "    # unquote\n",
    "    s = re_quotes.sub(\"\\\"\", s)\n",
    "    # remove non-printable characters\n",
    "    # TODO: does not work properly for U+200E (which is changed to E2)\n",
    "    # check with \"Q29841121\"     \"Alexander Howe<U+200E>\"\n",
    "    s = remove_control_chars(s)\n",
    "    return s\n",
    "\n",
    "def get_items(fname, sep='\\t'):\n",
    "    items = dict()\n",
    "    synonyms = dict()\n",
    "    # format: \"Q863081\"       \"Billy \"\"The Kid\"\" Emerson\"\n",
    "    with open(fname, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # extract parts\n",
    "            itemId, itemLabels = line.strip().split(sep, 1)\n",
    "            itemLabels = itemLabels.split(sep)\n",
    "            itemLabel = itemLabels.pop(0)\n",
    "            # remove surrounding quotes\n",
    "            itemId = clean(itemId)\n",
    "            itemLabel = clean(itemLabel)\n",
    "            # ignore duplicates\n",
    "            if itemLabel not in items:\n",
    "                items[itemLabel] = itemId\n",
    "            elif int(itemId[1:]) < int(items[itemLabel][1:]):\n",
    "                # ensure that we always use the item with the lowest id\n",
    "                items[itemLabel] = itemId\n",
    "            # handle synonyms\n",
    "            for syn in itemLabels:\n",
    "                syn = clean(syn)\n",
    "                # conditions as before (but merged with or)\n",
    "                if syn not in synonyms or int(itemId[1:]) < int(items[itemLabel][1:]):\n",
    "                    # we store the itemLabel such that we can easily\n",
    "                    # print it and get the corresponding itemId via\n",
    "                    # items\n",
    "                    synonyms[syn] = itemLabel\n",
    "    return items, synonyms\n",
    "\n",
    "### DEFACTORING NAMESPACE\n",
    "# Read list of persons from Wikidata\n",
    "items, synonyms = get_items(WIKIDATA_HUMANS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Code in dieser Zelle liest die Wikidata-Personenliste ein und überträgt die Einträge darin in ein Dictionary, sodass jeder ID der zugehörige Personenname zugeordnet wird. Außerdem wird ein weiteres Dictionary für die Synonyme angelegt. Dadurch können alle Namen und deren Synonyme eindeutig über die ID identifiziert werden.  \n",
    "Die beiden Methoden *remove_control_chars* und *clean* sind Hilfsmethoden für *get_items*. Sie entfernen Steuer- und Leerzeichen aus dem Datensatz, indem die zu Beginn des Programms definierten regulären Ausdrücke verwendet werden. Der Funktionsname *clean* deutet an, dass der Datensatz von den entfernten Zeichen gesäubert wird, weil sie für Menschen keine Bedeutung haben oder sogar stören, weil sie den Lesefluss unterbrechen. Für den Vergleich der Personennamen aus der Datenbank von Wikidata mit Textstellen der NYT-Artikel ist eine identische Zeichenfolge dagegen sehr wichtig.  \n",
    "Die Kommentare in der Methode  weisen darauf hin, dass diese Ausdrücke noch nicht für alle Steuerzeichen in der Personenliste funktionieren, weil sie nicht vollständig entfernt werden. Der nicht abgeschlossene Zustand des Programms spricht dafür, dass Fischer und Jäschke das Problem für vernachlässigbar halten. Es bleibt jedoch unklar, welche konkrete Auswirkung die fehlerhafte Bereinigung auf das Ergebnis der Trefferliste hat.    \n",
    "\n",
    "Die Methode *get_items* liest die Personenliste zeilenweise ein und erzeugt daraus zwei Dictionaries. Das Dictionary *items* speichert den Personennamen als Schlüssel und die dazugehörige ID als Wert. Diese Aufteilung erscheint auf den ersten Blick widersprüchlich, da sie die Bedeutung von ID und Entität umdreht. Die Erklärung dafür liefern Fischer und Jäschke im letzten Kommentar der Methode selbst (\"# we store the itemLabel such that we can easily print it and get the corresponding itemId via items\"). Sie ergibt sich aus der Kombination mit dem zweiten Dictionary, *synonyms*. Darin wird das Synonym für die Person als Schlüssel und der Personenname als Wert gespeichert. Da die Einträge von Dictionaries in Python über den Ausdruck *dictionary\\[schlüssel\\]* ausgegeben werden, kann durch die Zuordnung in *items* und *synonyms* problemlos eine Verknüpfung zwischen Synonymen und den zugehörigen IDs hergestellt werden, obwohl diese in verschiedenen Dictionaries gespeichert werden. Ein Beispiel dafür liefert die folgende Inspektionszelle.  \n",
    "Dass hier mit Daten von Wikidata gearbeitet wird, wird auch durch die Variablennamen deutlich. Die Bezeichnungen *itemId* und *itemLabel* sind Begriffe aus dem Wikidata Toolkit. Außerdem ist die Schreibweise im camelCase eine Konvention in Java und für Python unüblich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Maggie Thatcher\" ist ein Synonym für \"Margaret Thatcher\". Sie hat die ID Q7416.\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "\n",
    "synonym = 'Maggie Thatcher'\n",
    "\n",
    "print(f'\"{synonym}\" ist ein Synonym für \"{synonyms[synonym]}\". Sie hat die ID {items[synonyms[synonym]]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die zwei Dictionaries können alle Namen und deren Synonyme eindeutig über die ID identifiziert werden. Diese Personenliste wird jetzt herangezogen, um die erzeugte Liste potenzieller Vossanto-Treffer damit abzugleichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abgleich der Datensätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFACTORING FUNCTION\n",
    "\n",
    "def compare_lists(input_file, output_file):\n",
    "    with open(input_file) as f:\n",
    "        with open(output_file, 'w') as output_wd:\n",
    "            for line in f:\n",
    "                article, phrase, sentence = line.strip().split('\\t', 2)\n",
    "                # the John Doe of -> strip \"the\" and \"of\"\n",
    "                item = phrase[4:-3]\n",
    "                # check if exists\n",
    "                if item in items and item not in blacklist:\n",
    "                    print(article, items[item], phrase, item, item, sentence, file=output_wd, sep='\\t')\n",
    "                elif item in synonyms and item not in blacklist:\n",
    "                    print(article, items[synonyms[item]], phrase, item, synonyms[item], sentence, file=output_wd, sep='\\t')\n",
    "                else:\n",
    "                    # check if the phrase itself contains \"the\"\n",
    "                    for match in re_the.findall(item):\n",
    "                        if match in items and match not in blacklist:\n",
    "                            print(article, items[match], phrase, match, match, sentence, file=output_wd, sep='\\t')\n",
    "                        elif match in synonyms and match not in blacklist:\n",
    "                            print(article, items[synonyms[match]], phrase, match, synonyms[match], sentence, file=output_wd, sep='\\t')\n",
    "                            \n",
    "compare_lists('theof_1987_prep.tsv', 'theof_1987_wd.tsv')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Methode *compare_lists* existiert im Originalcode nicht. Stattdessen wird der darin verwendete Code in der Main-Methode des Skripts *check_wikidata.py* ausgeführt. In diesem Notebook wird der Code zweimal ausgeführt, um den Effekt der Blacklist auf das Ergebnis zu verdeutlichen. Für die Wiederverwendbarkeit wurde er daher in einer Methode gespeichert. Außerdem wird das Ergebnis des Codes im Original durch die Kommandozeile in eine Datei umgeleitet – hier wird die Ausgabedatei wie bei *print_matches* im print-Statement selbst festgelegt.  \n",
    "\n",
    "An dieser Stelle des Codes erfolgt der Vergleich von Vossanto-Treffern durch das Regex-Muster mit der Personenliste von Wikidata. Innerhalb der Methode erfolgt bereits der Abgleich mit der Blacklist (vgl. Code nach \"# check if exists\"). Sie ist zu diesem Zeitpunkt noch leer und hat dadurch keinen Einfluss auf das Ergebnis.  \n",
    "Die potenziellen Treffer werden um die Begriffe \"the\" und \"of\" gekürzt, damit nur der Name übrig bleibt. Anschließend wird überprüft, ob dieser Name entweder im Dictionary *items* oder *synonyms* vorkommt. Gleichzeitig wird überprüft, ob der Name nicht auf der Blacklist steht. Wenn beide Überprüfungen positiv verlaufen, wird der Treffer in die Ausgabedatei geschrieben.  \n",
    "Damit sind jedoch nicht alle Fälle abgedeckt. Die erste Abfrage funktioniert nur für Zeichenfolgen, bei denen innerhalb von \"the\" und \"of\" nur ein Personenname steht. Da das standardmäßig verwendete Regex-Muster jedoch bis zu fünf Wörter zwischen \"the\" und \"of\" zulässt, geht diese Einschränkung teilweise zu weit. Ein Beispiel dafür ist die Phrase \"the director and the writer of\". Auch mit der Kürzung verbleibt \"director and the writer\". Selbst wenn \"writer\" ein Personenname wäre, würde er durch die vorangehenden Begriffe nicht in der Personenliste von Wikidata gefunden werden. Daher wenden Fischer und Jäschke eine weitere Überprüfung an (vgl. Code nach '# check if the phrase itself contains \"the\"'): Die Phrase selbst wird nochmals nach \"the\" durchsucht. Für den darauf folgenden Begriff wird anschließend wieder geprüft, ob dieser in der Wikidata-Personenliste enthalten ist und nicht auf der Blacklist steht.  \n",
    "Hier wird die Schwierigkeit beim Umgang mit natürlicher Sprache sehr deutlich. Einfache Muster können die vielen Möglichkeiten bei der Verwendung natürlicher Sprache häufig nicht vollständig abdecken. Gleichzeitig haben sehr spezifische Muster den Nachteil, dass ggf. zu stark aussortiert wird. In solchen Fällen muss eine Abwägung zwischen Genauigkeit und Treffermenge getroffen werden.  \n",
    "\n",
    "Mit der nächsten Inspektion soll ein Überblick über den Effekt der Personenliste auf die identifizierten Treffer gegeben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5236 theof_1987_wd.tsv\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "!wc -l theof_1987_wd.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insgesamt enthält die Trefferliste noch 5.236 Einträge. Im Vergleich zu den 2.8 Millionen Einträgen aus dem Durchlauf ohne die Personenliste sind das deutlich weniger. Die nächste Zelle wird einen Eindruck geben, ob falsche Treffer damit in bedeutsamer Weise aussortiert werden konnten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    746 the House of\r\n",
      "    407 the Bureau of\r\n",
      "    374 the image of\r\n",
      "    312 the Court of\r\n",
      "    310 the wife of\r\n",
      "    220 the Church of\r\n",
      "    211 the Hall of\r\n",
      "    171 the Mayor of\r\n",
      "    120 the Bill of\r\n",
      "    108 the star of\r\n",
      "sort: Schreiben fehlgeschlagen: Standardausgabe: Datenübergabe unterbrochen (broken pipe)\r\n",
      "sort: Schreibfehler\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### Überblick über die häufigsten Treffer\n",
    "!awk -F'\\t' '{print $3}' theof_1987_wd.tsv| sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach wie vor enthält die Liste der zehn häufigsten Treffer keine richtigen Vossantos. Das liegt u. a. Problem darin, dass die Wikidataliste Personen mit Namen wie \"Hall\" oder \"Church\" enthält. Daher werden diese Begriffe nicht aussortiert. Fischer und Jäschke setzen daher zusätzlich auf eine Blacklist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schritt 3: Regex-Muster, Wikidata und Blacklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Erzeugung der Blacklist ist der einzige Schritt in der Durchführung der Vossanto-Studie, der nicht durch Code abgebildet ist. Daher ist ihr Zustandekommen nicht reproduzierbar. Fischer und Jäschke geben an, dass sie die Blacklist auf Basis der Treffer angelegt haben, die bei der Suche des Regex-Musters nach dem Abgleich mit der Personenliste gefunden wurden. Davon wurden jeweils die Begriffe auf die Blacklist gesetzt, bei denen die Wahrscheinlichkeit hoch ist, dass sie beim Vorkommen in den Artikeln keine Person beschreiben.  \n",
    "Durch dieses Vorgehen ist die Blacklist sehr stark von den Ausgangsdatensätzen sowie der Einschätzung der Studienautoren abhängig. Ersteres bedeutet für das Defactoring, dass es nicht möglich ist, mit einer neueren Version eines Datendumps von Wikidata zu arbeiten. Dieser würde viele Begriffe enthalten, die im Datensatz der Originalstudie noch nicht vorhanden waren – und damit auch nicht in die Blacklist übernommen wurden. Ein Beispiel dafür ist Andrew John Henry Way (mit der ID Q17641254), der vermutlich erst nach dem 19.07.2017 zur Datenbank von Wikidata hinzugefügt wurde. Zumindest steht dieser Name nicht in der Personenliste von Fischer und Jäschke. Das hat zur Folge, dass die Durchführung von *compare_lists* mit einem Datendump vom 02.12.2019 jedes Vorkommen der Phrase \"the way of\" als Treffer wertet. Daher wird neben der vorgegebenen Personenliste auch mit der von Fischer und Jäschke veröffentlichten Blacklist gearbeitet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erzeugung der Blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289 blacklist_prep.tsv\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "!wc -l blacklist_prep.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Blacklist enthält 1.289 Einträge. Da sie unter Berücksichtigung NYT-Artikel aller Jahrgänge von 1987-2007 angelegt wurde, enthält sie viele Begriffe, die in keinem der Texte von 1987 vorkommen. Das ist an dieser Stelle unproblematisch, da die Blacklist im Code von *compare_lists* nur nach Begriffen durchsucht wird, die vorher in einem Text gefunden wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House\r\n",
      "Church\r\n",
      "Hall\r\n",
      "Bill\r\n",
      "Freedom\r\n",
      "Governor\r\n",
      "Sultan\r\n",
      "Duke\r\n",
      "King\r\n",
      "Chancellor\r\n",
      "Prince\r\n",
      "Princess\r\n",
      "Life\r\n",
      "Spirit\r\n",
      "Kingdom\r\n",
      "awk: write failure (Broken pipe)\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### Überblick über die Blacklist\n",
    "\n",
    "!awk -F'\\t' '{print $1}' blacklist_prep.tsv| head -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Inspektionszelle zeigt die ersten 15 Einträge der Blacklist. Fischer und Jäschke weisen selbst darauf hin, dass davon bis auf 62 Ausnahmen alle Begriffe aus nur einem Wort bestehen. Zum Beleg dieser Behauptung durchsuchen sie die Tabelle mit dem Programm *grep* nach Begriffen, die Leerzeichen enthalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "!grep \" \" blacklist_prep.tsv | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die ersten Begriffe auf der Blacklist zeigen, dass eine Verwendung im Rahmen einer Vossanto eher unwahrscheinlich ist. Dennoch ist es denkbar, dass durch einen Begriff auf der Blacklist eine richtige Vossanto aussortiert wird. Durch die Blacklist werden falsche Treffer wie \"the Prince of Wales\" aussortiert. Gleichzeitig würden auch solche Fälle nicht berücksichtigt, in denen Bezug auf den Musiker genommen wird. Für die Unterscheidung der Verwendung von \"Prince\" als Adelstitel oder als Musikername ist Kontextwissen notwendig, welches durch maschinelle Verfahren nicht einfach erzeugt werden kann. Fischer und Jäschke treffen hier eine bewusste Abwägung zwischen der Berühmtheit einer Person bzw. der Wahrscheinlichkeit von deren Verwendung in einer Vossanto und der Häufigkeit falscher Treffer.  \n",
    "An dieser Stelle wäre eine genauere Auseinandersetzung mit dem Konzept der *Berühmtheit* spannend. Da Fischer und Jäschke keine Definition dafür bereitstellen und auch der Workflow zur Erzeugung der Blacklist nicht klar definiert ist, bleibt nur eine vage Vermutung. Hier zeigt sich die Schwierigkeit bei der Übertragung unscharfer gesellschaftlicher Konzepte in maschinell verarbeitbare Strukturen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blacklist(fname, sep='\\t'):\n",
    "    items = set()\n",
    "    # format: Lone Ranger\\t7\n",
    "    with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            # extract parts\n",
    "            item, count = line.strip().split(sep)\n",
    "            items.add(item)\n",
    "    return items\n",
    "\n",
    "### DEFACTORING NAMESPACE\n",
    "blacklist = get_blacklist(BLACKLIST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Methode *get_blacklist* wird die Blacklistdatei eingelesen und in einem Set gespeichert. Ein Set ist hier ein geeigneter Datentyp, da dadurch sichergestellt wird, dass Begriffe nicht mehrmals in der Liste auftauchen. Damit enthält die bereits verwendete Variable *blacklist* nun sämtliche Begriffe auf der Blacklist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abgleich der Datensätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_lists('theof_1987_prep.tsv', 'theof_1987_wd_bl.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Aufruf unterscheidet nur marginal vom vorherigen Aufruf der Methode *compare_lists*. Der Unterschied liegt nur in der dabei verwendeten Variable *blacklist* sowie der Ausgabedatei, welche zusätzlich das Kürzel \"_bl\" im Namen trägt. Die Auswirkungen der Blacklist auf das Ergebnis der Trefferliste wird in der folgenden Zelle untersucht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 theof_1987_wd_bl.tsv\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "!wc -l theof_1987_wd_bl.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Kombination der drei Schritte zur Identifizierung der Vossantos werden nurnoch 131 potenzielle Treffer gefunden. Die folgende Inspektion zeigt die häufigsten davon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4 Horatio Alger\r\n",
      "      4 Frank Sinatra\r\n",
      "      3 Woody Allen\r\n",
      "      3 Madonna\r\n",
      "      2 Tom Seaver\r\n",
      "      2 Pete Rose\r\n",
      "      2 Joan Baez\r\n",
      "      2 Jackie Robinson\r\n",
      "      2 Groucho Marx\r\n",
      "      2 Goebbels\r\n"
     ]
    }
   ],
   "source": [
    "### DEFACTORING INSPECTION\n",
    "### Überblick über die häufigsten Treffer\n",
    "!awk -F'\\t' '{print $4}' theof_1987_wd_bl.tsv| sort | uniq -c | sort -nr | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe der Blacklist zeigen die zehn häufigsten Treffer richtige Vossantos. Damit ist die Kombination aus Regex-Muster, Wikidataeinträgen und manueller Blacklist der beste der bisher geprüften Ansätze zur Identifizierung von Vossantos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darstellung der Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da sich die in *\"The Michael Jordan of greatness\". Extracting Vossian antonomasia from two decades of The New York Times, 1987-2007* dargestellten Ergebnisse auf die Analyse des Gesamtkorpus mit allen Artikeln beziehen, können diese mit dem Teilkorpus nicht nachvollzogen werden. Fischer und Jäschke konzentrieren sich bei der Besprechung der Ergebnisse auf folgende Kategorien:\n",
    "\n",
    "1. die häufigsten Vossanto SOURCEs\n",
    "2. die häufigsten Vossanto MODIFIER\n",
    "3. Rubriken der NYT, in denen Vossantos am häufigsten vorkommen\n",
    "4. Autorinnen und Autoren, die Vossantos besonders häufig verwenden\n",
    "\n",
    "Wegen der ausführlichen Dokumentation der Methode und weiterer Statistiken lassen sich zumindest die Ergebnisse der ersten Kategorie vergleichen. Die in der letzten Inspektionszelle gezeigte Übersicht über die 10 häufigsten Vossanto-Treffer entspricht den von Fischer und Jäschke veröffentlichten Zahlen für die Artikel aus dem Jahr 1987. \n",
    "Durch den veröffentlichten Code wäre es zwar möglich, auch für die Kategorien 2-4 Statistiken zu berechnen. Dafür müsste die Tabelle mit den Treffern zunächst in eine Datei des Auszeichnungsformats *.org* konvertiert werden. Anschließend haben Fischer und Jäschke die MODIFIER und andere Teile der Vossantos manuell gekennzeichnet. Auf Basis dieser Kennzeichnung kann die Datei anschließend analysiert und grafisch aufbereitet werden. Da für das Jahr 1987 keine Vergleichswerte vorliegen, ist eine grafische Darstellung der Ergebnisse an dieser Stelle nicht zielführend um die Studienergebnisse zu reproduzieren. Dieses Problem verdeutlicht einmal mehr die Abhängigkeit von Code und Daten."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Inhaltsverzeichnis",
   "title_sidebar": "Inhaltsverzeichnis",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "361px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
